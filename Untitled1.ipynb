{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVx7K0Qdid/nJ1rUqeG0de",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta12345as/manikanta12345asCODECLAUSEINTERNSHIP/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJfe-CWjc6Rr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "corpus = [\n",
        "    'NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.',\n",
        "    'NLP is used to analyze text, allowing machines to understand how humanâ€™s speak. This human-computer interaction enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is commonly used for text mining, machine translation, and automated question answering.',\n",
        "    'NLP algorithms are typically based on machine learning algorithms. Instead of hand-coding large sets of rules, NLP can rely on machine learning to automatically learn these rules by analyzing a set of examples (i.e. a large corpus, like a book, down to a collection of sentences), and making a stastical inference. In general, the more data analyzed, the more accurate the model will be.',\n",
        "]\n",
        "vectorizer = CountVectorizer(stop_words='english', min_df=2, binary=False)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "docterm = pd.DataFrame(X.toarray(), index=['Doc1', 'Doc2', 'Doc3'], columns=list(vectorizer.get_feature_names()))\n",
        "\n",
        "print(docterm.T)\n",
        "OUTPUT\n",
        "              Doc1  Doc2  Doc3\n",
        "analysis          1     1     0\n",
        "analyze           1     1     0\n",
        "automatic         1     1     0\n",
        "entity            1     1     0\n",
        "extraction        1     2     0\n",
        "human             1     2     0\n",
        "like              0     1     1\n",
        "machine           0     1     2\n",
        "named             1     1     0\n",
        "nlp               2     2     2\n",
        "recognition       2     1     0\n",
        "relationship      1     1     0\n",
        "sentiment         1     1     0\n",
        "speech            1     1     0\n",
        "summarization     1     1     0\n",
        "topic             1     1     0\n",
        "translation       1     1     0\n",
        "understand        1     1     0\n",
        ""
      ]
    }
  ]
}